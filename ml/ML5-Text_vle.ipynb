{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"http://www.bigdive.eu/wp-content/uploads/2012/05/logoBIGDIVE-01.png\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "# Text Analysis\n",
    "\n",
    "## André Panisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['stem']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "### The 20 Newsgroups data set\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups\n",
    "\n",
    "The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dataset = datasets.fetch_20newsgroups()\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print documents[0] # dobiamo rimuovere quello che non ci pu essere d'aiuto nell'analisi\n",
    "# vedi doc python regular expression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Removing from text pieces that do not convey any semantic meaning (e.g., mail headers, email adresses, host names...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from_re = re.compile(r\"^From: .*\\n\") # ^ dall'inizio della stringa che equivale all'inizio del documento\n",
    "\n",
    "print from_re.sub('', documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    documents[i] = from_re.sub('', documents[i])\n",
    "print documents[0]\n",
    "\n",
    "# sostituiamo il primo doc con quell in cui eliminiamo il From:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: create a regular expression to remove the Nntp-Posting-Host header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "host_re = re.compile(r\"\\nNntp-Posting-Host: .*\\n\") # \\n all'inizio di una linea\n",
    "# inseriamo \\n anche nella print, altrimenti il testo successivo viene sulla stessa linea della precedente\n",
    "print host_re.sub('\\n', documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: WHAT car is this!?\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    documents[i] = host_re.sub('\\n', documents[i])\n",
    "\n",
    "print documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text Messages to Feature Vectors\n",
    "We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n",
    "\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency\n",
    "\n",
    "We want to consider the relative importance of particular words, so we'll use term frequency–inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n",
    "\n",
    "## Mathematical details\n",
    "\n",
    "tf–idf is the product of two statistics, term frequency and inverse document\n",
    "frequency. Various ways for determining the exact values of both statistics\n",
    "exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n",
    "choice is to use the ''raw frequency'' of a term in a document, i.e. the\n",
    "number of times that term ''t'' occurs in document ''d''. If we denote the raw\n",
    "frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n",
    "tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n",
    "include:\n",
    "\n",
    "  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n",
    "  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n",
    "  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: :$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "The '''inverse document frequency''' is a measure of whether the term is\n",
    "common or rare across all documents. It is obtained by dividing the total\n",
    "number of documents by the number of documents containing the\n",
    "term, and then taking the logarithm of that quotient.\n",
    "\n",
    ":$ \\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$\n",
    "\n",
    "with\n",
    "\n",
    "  * $ |D| $: cardinality of D, or the total number of documents in the corpus \n",
    "  * $ |\\{d \\in D: t \\in d\\}| $ : number of documents where the term $ t $ appears (i.e., $ \\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n",
    "\n",
    "Mathematically the base of the log function does not matter and constitutes a\n",
    "constant multiplicative factor towards the overall result.\n",
    "\n",
    "Then tf–idf is calculated as\n",
    "\n",
    "$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text # subpackage del pacchetto feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fify',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.ENGLISH_STOP_WORDS # parole da escludere a priori sulla base del linguaggio scelto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = text.CountVectorizer(max_df=0.8, # escludo le parole che appaiono in piu' dell'80% dei doc \n",
    "                                  max_features=10000, # ne tiro fuori massimo 10.000\n",
    "                                  stop_words=text.ENGLISH_STOP_WORDS)\n",
    "# il parametro mindf (min doc freq) rimuove le aprole che ahnno una frequenza troppo bassa, ad esempio gli errori\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9629, 9268, 3280, 8936, 1866, 6248, 6956, 4492, 9313, 5696, 2226,\n",
       "       6625,  101, 9797, 3400, 7913, 2758, 3124, 8431, 5503, 5267, 3235,\n",
       "       1816, 3125, 7415, 8289,  791, 1741, 8068, 7624, 1584, 5182, 5957,\n",
       "       3387, 8402, 9936, 7102, 4422, 4730, 5504, 5604, 8913, 4610, 1699,\n",
       "       6180], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0].nonzero()[1] # counts contiene i doc, prendo il primo, prendo gli elementi che sono comparsi almeno\n",
    "# una volta (non zero), tralascio la prima variabile che  l'indice e restituisco la colonna [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10000 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 45 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0].data # la parola car dovrebbe essere la piu' freq --> verifica linee successive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'000',\n",
       " u'005',\n",
       " u'01',\n",
       " u'02',\n",
       " u'02238',\n",
       " u'02p',\n",
       " u'03',\n",
       " u'030',\n",
       " u'0358',\n",
       " u'04',\n",
       " u'040',\n",
       " u'0400',\n",
       " u'05',\n",
       " u'06',\n",
       " u'07',\n",
       " u'08',\n",
       " u'09',\n",
       " u'0b',\n",
       " u'0c',\n",
       " u'0d',\n",
       " u'0el',\n",
       " u'0em',\n",
       " u'0g',\n",
       " u'0i',\n",
       " u'0l',\n",
       " u'0m',\n",
       " u'0p',\n",
       " u'0q',\n",
       " u'0qax',\n",
       " u'0t',\n",
       " u'0tbxn',\n",
       " u'0tq',\n",
       " u'0u',\n",
       " u'0w',\n",
       " u'10',\n",
       " u'100',\n",
       " u'1000',\n",
       " u'101',\n",
       " u'102',\n",
       " u'1024',\n",
       " u'1024x768',\n",
       " u'102nd',\n",
       " u'103',\n",
       " u'104',\n",
       " u'105',\n",
       " u'106',\n",
       " u'107',\n",
       " u'108',\n",
       " u'109',\n",
       " u'10k',\n",
       " u'10th',\n",
       " u'11',\n",
       " u'110',\n",
       " u'1100',\n",
       " u'111',\n",
       " u'112',\n",
       " u'113',\n",
       " u'114',\n",
       " u'115',\n",
       " u'117',\n",
       " u'118',\n",
       " u'119',\n",
       " u'12',\n",
       " u'120',\n",
       " u'1200',\n",
       " u'121',\n",
       " u'122',\n",
       " u'123',\n",
       " u'124',\n",
       " u'125',\n",
       " u'126',\n",
       " u'127',\n",
       " u'128',\n",
       " u'1280x1024',\n",
       " u'129',\n",
       " u'13',\n",
       " u'130',\n",
       " u'131',\n",
       " u'132',\n",
       " u'133',\n",
       " u'134',\n",
       " u'135',\n",
       " u'136',\n",
       " u'137',\n",
       " u'138',\n",
       " u'139',\n",
       " u'13p',\n",
       " u'13q',\n",
       " u'13s',\n",
       " u'14',\n",
       " u'140',\n",
       " u'141',\n",
       " u'142',\n",
       " u'143',\n",
       " u'144',\n",
       " u'145',\n",
       " u'146',\n",
       " u'147',\n",
       " u'148',\n",
       " u'149',\n",
       " u'15',\n",
       " u'150',\n",
       " u'1500',\n",
       " u'151',\n",
       " u'152',\n",
       " u'153',\n",
       " u'1542',\n",
       " u'155',\n",
       " u'156',\n",
       " u'157',\n",
       " u'158',\n",
       " u'159',\n",
       " u'15o',\n",
       " u'16',\n",
       " u'160',\n",
       " u'161',\n",
       " u'162',\n",
       " u'163',\n",
       " u'164',\n",
       " u'165',\n",
       " u'16550',\n",
       " u'167',\n",
       " u'169',\n",
       " u'16m',\n",
       " u'16mb',\n",
       " u'17',\n",
       " u'170',\n",
       " u'172',\n",
       " u'174',\n",
       " u'175',\n",
       " u'177',\n",
       " u'18',\n",
       " u'180',\n",
       " u'182',\n",
       " u'184',\n",
       " u'185',\n",
       " u'187',\n",
       " u'188',\n",
       " u'19',\n",
       " u'190',\n",
       " u'1900',\n",
       " u'1914',\n",
       " u'1915',\n",
       " u'1918',\n",
       " u'1919',\n",
       " u'192',\n",
       " u'1920',\n",
       " u'1923',\n",
       " u'1934',\n",
       " u'1948',\n",
       " u'195',\n",
       " u'1950',\n",
       " u'1958',\n",
       " u'196',\n",
       " u'1960',\n",
       " u'1963',\n",
       " u'1964',\n",
       " u'1967',\n",
       " u'1968',\n",
       " u'1969',\n",
       " u'1970',\n",
       " u'1972',\n",
       " u'1973',\n",
       " u'1974',\n",
       " u'1975',\n",
       " u'1976',\n",
       " u'1977',\n",
       " u'1978',\n",
       " u'1979',\n",
       " u'198',\n",
       " u'1980',\n",
       " u'1981',\n",
       " u'1982',\n",
       " u'1983',\n",
       " u'1984',\n",
       " u'1985',\n",
       " u'1986',\n",
       " u'1987',\n",
       " u'1988',\n",
       " u'1989',\n",
       " u'199',\n",
       " u'1990',\n",
       " u'1991',\n",
       " u'1992',\n",
       " u'1993',\n",
       " u'1993apr12',\n",
       " u'1993apr13',\n",
       " u'1993apr14',\n",
       " u'1993apr15',\n",
       " u'1993apr16',\n",
       " u'1993apr17',\n",
       " u'1993apr18',\n",
       " u'1993apr19',\n",
       " u'1993apr2',\n",
       " u'1993apr20',\n",
       " u'1993apr21',\n",
       " u'1993apr22',\n",
       " u'1993apr23',\n",
       " u'1993apr3',\n",
       " u'1993apr4',\n",
       " u'1993apr5',\n",
       " u'1993apr6',\n",
       " u'1994',\n",
       " u'1a',\n",
       " u'1b',\n",
       " u'1d',\n",
       " u'1d9',\n",
       " u'1d9l',\n",
       " u'1e',\n",
       " u'1eq',\n",
       " u'1f',\n",
       " u'1f9',\n",
       " u'1fpl',\n",
       " u'1g',\n",
       " u'1j',\n",
       " u'1k',\n",
       " u'1m',\n",
       " u'1mb',\n",
       " u'1p',\n",
       " u'1q',\n",
       " u'1qy',\n",
       " u'1s',\n",
       " u'1st',\n",
       " u'1t',\n",
       " u'1v',\n",
       " u'1w',\n",
       " u'1x',\n",
       " u'1y',\n",
       " u'1z4',\n",
       " u'1z6e',\n",
       " u'20',\n",
       " u'200',\n",
       " u'2000',\n",
       " u'201',\n",
       " u'202',\n",
       " u'203',\n",
       " u'204',\n",
       " u'2048',\n",
       " u'205',\n",
       " u'206',\n",
       " u'207',\n",
       " u'20mb',\n",
       " u'20th',\n",
       " u'21',\n",
       " u'210',\n",
       " u'211',\n",
       " u'212',\n",
       " u'213',\n",
       " u'214',\n",
       " u'215',\n",
       " u'216',\n",
       " u'217',\n",
       " u'219',\n",
       " u'22',\n",
       " u'220',\n",
       " u'221',\n",
       " u'222',\n",
       " u'223',\n",
       " u'224',\n",
       " u'225',\n",
       " u'226',\n",
       " u'227',\n",
       " u'23',\n",
       " u'230',\n",
       " u'231',\n",
       " u'232',\n",
       " u'239',\n",
       " u'24',\n",
       " u'240',\n",
       " u'2400',\n",
       " u'241',\n",
       " u'244',\n",
       " u'245',\n",
       " u'246',\n",
       " u'248',\n",
       " u'249',\n",
       " u'24bit',\n",
       " u'24e',\n",
       " u'24x',\n",
       " u'25',\n",
       " u'250',\n",
       " u'251',\n",
       " u'252',\n",
       " u'253',\n",
       " u'254',\n",
       " u'255',\n",
       " u'256',\n",
       " u'256k',\n",
       " u'25mhz',\n",
       " u'26',\n",
       " u'267',\n",
       " u'268',\n",
       " u'27',\n",
       " u'270',\n",
       " u'273',\n",
       " u'274',\n",
       " u'275',\n",
       " u'278',\n",
       " u'28',\n",
       " u'280',\n",
       " u'282',\n",
       " u'285',\n",
       " u'286',\n",
       " u'287',\n",
       " u'29',\n",
       " u'2923',\n",
       " u'294',\n",
       " u'296',\n",
       " u'2_',\n",
       " u'2a',\n",
       " u'2b',\n",
       " u'2d',\n",
       " u'2di',\n",
       " u'2e',\n",
       " u'2f',\n",
       " u'2g',\n",
       " u'2j',\n",
       " u'2l',\n",
       " u'2m',\n",
       " u'2mb',\n",
       " u'2nd',\n",
       " u'2p',\n",
       " u'2pl',\n",
       " u'2pu',\n",
       " u'2q',\n",
       " u'2r_',\n",
       " u'2s',\n",
       " u'2tct',\n",
       " u'2tg',\n",
       " u'2tm',\n",
       " u'2u',\n",
       " u'2w',\n",
       " u'2x',\n",
       " u'30',\n",
       " u'300',\n",
       " u'3000',\n",
       " u'301',\n",
       " u'303',\n",
       " u'30602',\n",
       " u'31',\n",
       " u'310',\n",
       " u'312',\n",
       " u'313',\n",
       " u'314',\n",
       " u'316',\n",
       " u'32',\n",
       " u'320',\n",
       " u'323',\n",
       " u'325',\n",
       " u'32bis',\n",
       " u'33',\n",
       " u'330',\n",
       " u'333',\n",
       " u'33mhz',\n",
       " u'34',\n",
       " u'345',\n",
       " u'348',\n",
       " u'34l',\n",
       " u'34u',\n",
       " u'35',\n",
       " u'350',\n",
       " u'353',\n",
       " u'3539',\n",
       " u'354',\n",
       " u'357',\n",
       " u'36',\n",
       " u'360',\n",
       " u'364',\n",
       " u'37',\n",
       " u'375',\n",
       " u'38',\n",
       " u'382761',\n",
       " u'386',\n",
       " u'386bsd',\n",
       " u'386sx',\n",
       " u'39',\n",
       " u'3a',\n",
       " u'3b',\n",
       " u'3c',\n",
       " u'3com',\n",
       " u'3d',\n",
       " u'3do',\n",
       " u'3dy',\n",
       " u'3e',\n",
       " u'3h',\n",
       " u'3hz',\n",
       " u'3k',\n",
       " u'3l',\n",
       " u'3m',\n",
       " u'3n',\n",
       " u'3o',\n",
       " u'3p',\n",
       " u'3q',\n",
       " u'3rd',\n",
       " u'3t',\n",
       " u'3v9f0',\n",
       " u'3w2tm',\n",
       " u'3x',\n",
       " u'40',\n",
       " u'400',\n",
       " u'4000',\n",
       " u'403',\n",
       " u'405',\n",
       " u'407',\n",
       " u'408',\n",
       " u'41',\n",
       " u'410',\n",
       " u'412',\n",
       " u'415',\n",
       " u'416',\n",
       " u'42',\n",
       " u'427',\n",
       " u'43',\n",
       " u'44',\n",
       " u'444',\n",
       " u'45',\n",
       " u'450',\n",
       " u'455',\n",
       " u'456',\n",
       " u'46',\n",
       " u'460',\n",
       " u'462',\n",
       " u'47',\n",
       " u'48',\n",
       " u'480',\n",
       " u'486',\n",
       " u'486dx',\n",
       " u'486dx2',\n",
       " u'49',\n",
       " u'492',\n",
       " u'4c',\n",
       " u'4d',\n",
       " u'4e',\n",
       " u'4f',\n",
       " u'4k',\n",
       " u'4l',\n",
       " u'4m',\n",
       " u'4mb',\n",
       " u'4p',\n",
       " u'4r',\n",
       " u'4t',\n",
       " u'4th',\n",
       " u'4u',\n",
       " u'50',\n",
       " u'500',\n",
       " u'5000',\n",
       " u'503',\n",
       " u'508',\n",
       " u'50mhz',\n",
       " u'51',\n",
       " u'510',\n",
       " u'512',\n",
       " u'512k',\n",
       " u'515',\n",
       " u'518',\n",
       " u'52',\n",
       " u'520',\n",
       " u'525',\n",
       " u'53',\n",
       " u'54',\n",
       " u'542',\n",
       " u'545',\n",
       " u'54715',\n",
       " u'55',\n",
       " u'550',\n",
       " u'56',\n",
       " u'57',\n",
       " u'571',\n",
       " u'575',\n",
       " u'58',\n",
       " u'580',\n",
       " u'59',\n",
       " u'5g',\n",
       " u'5g9p',\n",
       " u'5g9v',\n",
       " u'5j',\n",
       " u'5k',\n",
       " u'5m',\n",
       " u'5mb',\n",
       " u'5s',\n",
       " u'5t',\n",
       " u'5th',\n",
       " u'5u',\n",
       " u'5v',\n",
       " u'5w',\n",
       " u'60',\n",
       " u'600',\n",
       " u'6000',\n",
       " u'601',\n",
       " u'602',\n",
       " u'604',\n",
       " u'61',\n",
       " u'610',\n",
       " u'612',\n",
       " u'613',\n",
       " u'614',\n",
       " u'617',\n",
       " u'619',\n",
       " u'62',\n",
       " u'621',\n",
       " u'624',\n",
       " u'629',\n",
       " u'63',\n",
       " u'64',\n",
       " u'640x480',\n",
       " u'647',\n",
       " u'64k',\n",
       " u'65',\n",
       " u'650',\n",
       " u'66',\n",
       " u'663',\n",
       " u'666',\n",
       " u'67',\n",
       " u'675',\n",
       " u'68',\n",
       " u'68000',\n",
       " u'68030',\n",
       " u'68040',\n",
       " u'685',\n",
       " u'69',\n",
       " u'6_',\n",
       " u'6a',\n",
       " u'6c',\n",
       " u'6e',\n",
       " u'6e1t',\n",
       " u'6ei',\n",
       " u'6ei4',\n",
       " u'6ej',\n",
       " u'6f',\n",
       " u'6g',\n",
       " u'6k',\n",
       " u'6l',\n",
       " u'6n',\n",
       " u'6p',\n",
       " u'6ql',\n",
       " u'6t',\n",
       " u'6th',\n",
       " u'6u',\n",
       " u'6um',\n",
       " u'6umu',\n",
       " u'6v',\n",
       " u'6w',\n",
       " u'6x',\n",
       " u'70',\n",
       " u'700',\n",
       " u'7000',\n",
       " u'703',\n",
       " u'706',\n",
       " u'708',\n",
       " u'71',\n",
       " u'713',\n",
       " u'714',\n",
       " u'72',\n",
       " u'73',\n",
       " u'730',\n",
       " u'734',\n",
       " u'74',\n",
       " u'7415',\n",
       " u'75',\n",
       " u'750',\n",
       " u'75di',\n",
       " u'75u',\n",
       " u'76',\n",
       " u'768',\n",
       " u'77',\n",
       " u'78',\n",
       " u'79',\n",
       " u'7b',\n",
       " u'7ex',\n",
       " u'7ey',\n",
       " u'7ez',\n",
       " u'7g',\n",
       " u'7klj',\n",
       " u'7kn',\n",
       " u'7r',\n",
       " u'7t',\n",
       " u'7th',\n",
       " u'7tl',\n",
       " u'7tu',\n",
       " u'7u',\n",
       " u'7v',\n",
       " u'7z',\n",
       " u'80',\n",
       " u'800',\n",
       " u'8000',\n",
       " u'800x600',\n",
       " u'801',\n",
       " u'804',\n",
       " u'805',\n",
       " u'8051',\n",
       " u'80ns',\n",
       " u'81',\n",
       " u'818',\n",
       " u'82',\n",
       " u'83',\n",
       " u'838',\n",
       " u'84',\n",
       " u'85',\n",
       " u'850',\n",
       " u'8514',\n",
       " u'86',\n",
       " u'87',\n",
       " u'88',\n",
       " u'89',\n",
       " u'891',\n",
       " u'8bit',\n",
       " u'8c',\n",
       " u'8c_',\n",
       " u'8k',\n",
       " u'8mb',\n",
       " u'8n',\n",
       " u'8th',\n",
       " u'8v',\n",
       " u'90',\n",
       " u'900',\n",
       " u'9000',\n",
       " u'908',\n",
       " u'91',\n",
       " u'91109',\n",
       " u'919',\n",
       " u'92',\n",
       " u'922',\n",
       " u'93',\n",
       " u'93105',\n",
       " u'94',\n",
       " u'94305',\n",
       " u'949',\n",
       " u'95',\n",
       " u'950',\n",
       " u'96',\n",
       " u'9600',\n",
       " u'97',\n",
       " u'9760',\n",
       " u'98',\n",
       " u'99',\n",
       " u'9937',\n",
       " u'9c',\n",
       " u'9d',\n",
       " u'9f',\n",
       " u'9f8',\n",
       " u'9f9',\n",
       " u'9f9f',\n",
       " u'9h',\n",
       " u'9l',\n",
       " u'9l3',\n",
       " u'9m',\n",
       " u'9p',\n",
       " u'9s',\n",
       " u'9v',\n",
       " u'9z',\n",
       " u'__',\n",
       " u'___',\n",
       " u'____',\n",
       " u'_____',\n",
       " u'______',\n",
       " u'_______',\n",
       " u'________',\n",
       " u'_________',\n",
       " u'__________',\n",
       " u'_______________________________________________________________________________',\n",
       " u'________________________________________________________________________________',\n",
       " u'_d',\n",
       " u'_is_',\n",
       " u'_l',\n",
       " u'_lw',\n",
       " u'_n',\n",
       " u'_not_',\n",
       " u'_o',\n",
       " u'_q',\n",
       " u'_the',\n",
       " u'a1',\n",
       " u'a2',\n",
       " u'a3',\n",
       " u'a4',\n",
       " u'a7',\n",
       " u'a85',\n",
       " u'a86',\n",
       " u'a865',\n",
       " u'a86r',\n",
       " u'a945',\n",
       " u'aa',\n",
       " u'aaa',\n",
       " u'aaron',\n",
       " u'aau',\n",
       " u'ab',\n",
       " u'ab4z',\n",
       " u'abandoned',\n",
       " u'abc',\n",
       " u'aber',\n",
       " u'abiding',\n",
       " u'abilities',\n",
       " u'ability',\n",
       " u'able',\n",
       " u'abo',\n",
       " u'abolish',\n",
       " u'abort',\n",
       " u'abortion',\n",
       " u'abraham',\n",
       " u'abroad',\n",
       " u'abs',\n",
       " u'absence',\n",
       " u'absolute',\n",
       " u'absolutely',\n",
       " u'absolutes',\n",
       " u'abstinence',\n",
       " u'abstract',\n",
       " u'abstracts',\n",
       " u'absurd',\n",
       " u'abu',\n",
       " u'abuse',\n",
       " u'abused',\n",
       " u'abuses',\n",
       " u'ac',\n",
       " u'acad',\n",
       " u'acad3',\n",
       " u'academic',\n",
       " u'academy',\n",
       " u'acc',\n",
       " u'accelerate',\n",
       " u'accelerated',\n",
       " u'acceleration',\n",
       " u'accelerator',\n",
       " u'accelerators',\n",
       " u'accept',\n",
       " u'acceptable',\n",
       " u'acceptance',\n",
       " u'accepted',\n",
       " u'accepting',\n",
       " u'accepts',\n",
       " u'access',\n",
       " u'accessible',\n",
       " u'accessories',\n",
       " u'accident',\n",
       " u'accidental',\n",
       " u'accidents',\n",
       " u'accomplish',\n",
       " u'accomplished',\n",
       " u'accord',\n",
       " u'according',\n",
       " u'account',\n",
       " u'accounting',\n",
       " u'accounts',\n",
       " u'accuracy',\n",
       " u'accurate',\n",
       " u'accurately',\n",
       " u'accuse',\n",
       " u'accused',\n",
       " u'ace',\n",
       " u'achieve',\n",
       " u'achieved',\n",
       " u'acid',\n",
       " u'acker',\n",
       " u'acknowledge',\n",
       " u'acknowledged',\n",
       " u'aclu',\n",
       " u'acm',\n",
       " u'acns',\n",
       " u'acpub',\n",
       " u'acquire',\n",
       " u'acquired',\n",
       " u'acquisition',\n",
       " u'acs',\n",
       " u'acsc',\n",
       " u'acsu',\n",
       " u'act',\n",
       " u'acting',\n",
       " u'action',\n",
       " u'actions',\n",
       " u'active',\n",
       " u'actively',\n",
       " u'activists',\n",
       " u'activities',\n",
       " u'activity',\n",
       " u'acts',\n",
       " u'actual',\n",
       " u'actually',\n",
       " u'acute',\n",
       " u'ad',\n",
       " u'adam',\n",
       " u'adams',\n",
       " u'adaptec',\n",
       " u'adapted',\n",
       " u'adapter',\n",
       " u'adapters',\n",
       " u'adaptor',\n",
       " u'adb',\n",
       " u'adcom',\n",
       " u'add',\n",
       " u'added',\n",
       " u'adding',\n",
       " u'addition',\n",
       " u'additional',\n",
       " u'additionally',\n",
       " u'additions',\n",
       " u'address',\n",
       " u'addressed',\n",
       " u'addresses',\n",
       " u'addressing',\n",
       " u'adds',\n",
       " u'adelaide',\n",
       " u'adequate',\n",
       " u'adirondack',\n",
       " u'adjust',\n",
       " u'adjusted',\n",
       " u'adl',\n",
       " u'admin',\n",
       " u'administration',\n",
       " u'administrative',\n",
       " u'administrator',\n",
       " u'admiral',\n",
       " u'admit',\n",
       " u'admitted',\n",
       " u'admittedly',\n",
       " u'adobe',\n",
       " u'adopt',\n",
       " u'adopted',\n",
       " u'adrian',\n",
       " u'ads',\n",
       " u'adult',\n",
       " u'adults',\n",
       " u'advance',\n",
       " u'advanced',\n",
       " u'advances',\n",
       " u'advantage',\n",
       " u'advantages',\n",
       " u'advertised',\n",
       " u'advertising',\n",
       " u'advice',\n",
       " u'advisory',\n",
       " u'advocate',\n",
       " u'advocates',\n",
       " u'ae',\n",
       " u'aero',\n",
       " u'aerospace',\n",
       " u'af',\n",
       " u'affair',\n",
       " u'affairs',\n",
       " u'affect',\n",
       " u'affected',\n",
       " u'afford',\n",
       " u'afraid',\n",
       " u'africa',\n",
       " u'african',\n",
       " u'afterlife',\n",
       " u'aftermarket',\n",
       " u'afternoon',\n",
       " u'ag',\n",
       " u'agate',\n",
       " u'agdam',\n",
       " u'age',\n",
       " u'aged',\n",
       " u'agencies',\n",
       " u'agency',\n",
       " u'agenda',\n",
       " u'agent',\n",
       " u'agents',\n",
       " u'ages',\n",
       " u'aggression',\n",
       " u'aggressive',\n",
       " u'agnostic',\n",
       " u'ago',\n",
       " u'agree',\n",
       " u'agreed',\n",
       " u'agreement',\n",
       " u'agreements',\n",
       " u'agrees',\n",
       " u'ah',\n",
       " u'ahead',\n",
       " u'ahf',\n",
       " u'ahl',\n",
       " u'ahmed',\n",
       " u'ahmet',\n",
       " u'ai',\n",
       " u'aid',\n",
       " u'aids',\n",
       " u'aim',\n",
       " u'aimed',\n",
       " u'ain',\n",
       " u'air',\n",
       " u'aircraft',\n",
       " u'airplane',\n",
       " u'airport',\n",
       " u'aisun3',\n",
       " u'aix',\n",
       " u'aj',\n",
       " u'ajr',\n",
       " u'ajz',\n",
       " u'ak',\n",
       " u'ak296',\n",
       " u'aka',\n",
       " u'al',\n",
       " u'ala',\n",
       " u'alabama',\n",
       " u'alan',\n",
       " u'alarm',\n",
       " u'alas',\n",
       " u'alaska',\n",
       " u'albany',\n",
       " u'albert',\n",
       " u'alberta',\n",
       " u'albicans',\n",
       " u'alchemy',\n",
       " u'alcohol',\n",
       " u'alert',\n",
       " u'alex',\n",
       " u'alexander',\n",
       " u'alexia',\n",
       " u'alfred',\n",
       " u'algorithm',\n",
       " u'algorithms',\n",
       " u'ali',\n",
       " u'alias',\n",
       " u'alice',\n",
       " u'alicea',\n",
       " u'alien',\n",
       " u'alignment',\n",
       " u'alike',\n",
       " u'alink',\n",
       " u'alive',\n",
       " u'allah',\n",
       " u'allan',\n",
       " u'alleg',\n",
       " u'alleged',\n",
       " u'allen',\n",
       " u'allergic',\n",
       " u'allergy',\n",
       " u'allies',\n",
       " u'allocated',\n",
       " u'allocation',\n",
       " u'allow',\n",
       " u'allowed',\n",
       " u'allowing',\n",
       " u'allows',\n",
       " u'almanac',\n",
       " u'alomar',\n",
       " u'alot',\n",
       " u'alpha',\n",
       " u'alt',\n",
       " u'alter',\n",
       " u'altered',\n",
       " u'alternate',\n",
       " u'alternative',\n",
       " u'alternatives',\n",
       " u'altitude',\n",
       " u'altogether',\n",
       " u'aludra',\n",
       " u'aluminum',\n",
       " u'alvin',\n",
       " u'ama',\n",
       " u'amanda',\n",
       " u'amateur',\n",
       " u'amazed',\n",
       " u'amazing',\n",
       " u'ambulance',\n",
       " u'amd',\n",
       " u'amdahl',\n",
       " u'amehdi',\n",
       " u'amend',\n",
       " u'amendment',\n",
       " u'america',\n",
       " u'american',\n",
       " u'americans',\n",
       " u'ames',\n",
       " u'amherst',\n",
       " u'ami',\n",
       " u'amiga',\n",
       " u'amino',\n",
       " u'amir',\n",
       " u'ammo',\n",
       " u'ammunition',\n",
       " u'amolitor',\n",
       " u'amounts',\n",
       " u'amour',\n",
       " u'amp',\n",
       " u'amplifier',\n",
       " u'amps',\n",
       " u'amsterdam',\n",
       " u'amusing',\n",
       " u'analog',\n",
       " u'analogy',\n",
       " u'analysis',\n",
       " u'analyst',\n",
       " u'anania',\n",
       " u'anas',\n",
       " u'anatolia',\n",
       " u'anchor',\n",
       " u'ancient',\n",
       " u'anderson',\n",
       " u'andersson',\n",
       " u'andi',\n",
       " u'andre',\n",
       " u'andre_beck',\n",
       " u'andreas',\n",
       " u'andrew',\n",
       " u'andy',\n",
       " u'anecdotal',\n",
       " u'anecdote',\n",
       " u'angel',\n",
       " u'angeles',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() # si pu' fare pulizia sui primi termini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'car', u'university', u'maryland', u'college', u'park', u'15',\n",
       "       u'wondering', u'enlighten', u'saw', u'day', u'door', u'sports',\n",
       "       u'looked', u'late', u'early', u'70s', u'called', u'doors',\n",
       "       u'really', u'small', u'addition', u'bumper', u'separate', u'rest',\n",
       "       u'body', u'know', u'model', u'engine', u'specs', u'years',\n",
       "       u'production', u'history', u'info', u'looking', u'mail', u'thanks',\n",
       "       u'il', u'brought', u'neighborhood'], \n",
       "      dtype='<U80')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(vectorizer.get_feature_names())[counts[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing with NLTK\n",
    "\n",
    "NLTK (Natural Language ToolKit) is a library for symbolic and statistical natural language processing (NLP).\n",
    "\n",
    "It supports a few functionalities for NLP, such as:\n",
    "\n",
    "- Lexical analysis: Word and text tokenizer\n",
    "- n-gram and collocations\n",
    "- Part-of-speech tagger\n",
    "- Tree model and Text chunker for capturing\n",
    "- Named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import stem\n",
    "import re\n",
    "\n",
    "pattern = re.compile('(?u)\\\\b[A-Za-z]{3,}')\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "def stemming(doc):\n",
    "    l = [stemmer.stem(t) for t in pattern.findall(doc)]\n",
    "    return [w for w in l if len(w) > 2]\n",
    "\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "def lemmatize(doc):\n",
    "    \n",
    "    def lemma(w):\n",
    "        l = wnl.lemmatize(t, wn.NOUN)\n",
    "        if l == w:\n",
    "            l = wnl.lemmatize(t, wn.ADJ)\n",
    "        if l == w:\n",
    "            l = wnl.lemmatize(t, wn.ADV)\n",
    "        if l == w:\n",
    "            l = wnl.lemmatize(t, wn.VERB)\n",
    "        return l\n",
    "    \n",
    "    l = [lemma(t) for t in pattern.findall(doc)]\n",
    "    return [w for w in l if len(w) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bigdive/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For grammatical reasons, documents are going to use different forms of a word,\n",
      "such as organize, organizes, and organizing.\n",
      "Additionally, there are families of derivationally related words with similar meanings,\n",
      "such as democracy, democratic, and democratization.\n",
      "\n",
      "[u'for', u'grammat', u'reason', u'document', u'are', u'use', u'differ', u'form', u'word', u'such', u'organ', u'organ', u'and', u'organ', u'addit', u'there', u'are', u'famili', u'deriv', u'relat', u'word', u'with', u'similar', u'mean', u'such', u'democraci', u'democrat', u'and', u'democrat']\n",
      "\n",
      "['For', 'grammatical', u'reason', u'document', 'use', 'different', u'form', 'word', 'such', 'organize', u'organize', 'and', u'organize', 'Additionally', 'there', u'family', 'derivationally', u'relate', u'word', 'with', 'similar', u'meaning', 'such', 'democracy', 'democratic', 'and', 'democratization']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"For grammatical reasons, documents are going to use different forms of a word,\n",
    "such as organize, organizes, and organizing.\n",
    "Additionally, there are families of derivationally related words with similar meanings,\n",
    "such as democracy, democratic, and democratization.\"\"\"\n",
    "\n",
    "print sentence\n",
    "print\n",
    "print stemming(sentence)\n",
    "print\n",
    "print lemmatize(sentence) # invece di troncare le parole, le normalizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = text.CountVectorizer(max_df=0.95, \n",
    "                                  max_features=10000, \n",
    "                                  stop_words='english',\n",
    "                                  encoding='latin1', \n",
    "                                  tokenizer=lemmatize, \n",
    "                                  ngram_range=(1, 2))\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'wam', u'umd', u'edu', u'thing', u'car', u'nntp', u'post', u'host',\n",
       "       u'university', u'maryland', u'college', u'park', u'wonder',\n",
       "       u'enlighten', u'saw', u'day', u'door', u'sport', u'look', u'late',\n",
       "       u'early', u'really', u'small', u'addition', u'bumper', u'separate',\n",
       "       u'rest', u'body', u'know', u'model', u'engine', u'spec', u'year',\n",
       "       u'production', u'make', u'history', u'info', u'mail', u'thank',\n",
       "       u'bring', u'neighborhood', u'wam umd', u'umd edu', u'subject car',\n",
       "       u'nntp post', u'post host', u'edu organization',\n",
       "       u'organization university', u'university maryland',\n",
       "       u'maryland college', u'college park', u'park line', u'sport car',\n",
       "       u'mail thank'], \n",
       "      dtype='<U26')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(vectorizer.get_feature_names())[counts[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a variable, tfidf, which is a vectorizer responsible for performing three important steps:\n",
    "\n",
    "- First, it will build a dictionary of features where keys are terms and values are indices of the term in the feature matrix (that's the fit part in fit_transform)\n",
    "- Second, it will transform our documents into numerical feature vectors according to the frequency of words appearing in each text message. Since any one text message is short, each feature vector will be made up of mostly zeros, each of which indicates that a given word appeared zero times in that message.\n",
    "- Lastly, it will compute the tf-idf weights for our term frequency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative Matrix Factorization for Topic extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine having 5 documents, 2 of them about environment and 2 of them about U.S. Congress and 1 about both, that means it says about government legislation process in protecting an environment. We need to write a program that unmistakably identifies category of each document and also returns a degree of belonging of each document to a particular category. For this elementary example we limit our vocabulary to 5 words: AIR, WATER, POLLUTION, DEMOCRAT, REPUBLICAN. Category ENVIRONMENT and category CONGRESS may contain all 5 words but with different probability. We understand that the word POLLUTION has more chances to be in the article about ENVIRONMENT than in the article about CONGRESS, but can theoretically be in both. Presume after an examination of our data we built following document-term table:\n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>document/word</td>\n",
    "<td>air</td>\n",
    "<td>water</td>\n",
    "<td>pollution</td>\n",
    "<td>democrat</td>\n",
    "<td>republican</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 1</td>\n",
    "<td>3</td>\n",
    "<td>2</td>\n",
    "<td>8</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 2</td>\n",
    "<td>1</td>\n",
    "<td>4</td>\n",
    "<td>12</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 3</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>10</td>\n",
    "<td>11</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 4</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>8</td>\n",
    "<td>5</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 5</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We distinguish our categories by the group of words assigned to them. We decide that category ENVIRONMENT normally should contain only words AIR, WATER, POLLUTION and category CONGRESS should contain only words DEMOCRAT and REPUBLICAN. We build another matrix, each row of which represent category and contains counts for only words that assigned to each category. \n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>categories</td>\n",
    "<td>air</td>\n",
    "<td>water</td>\n",
    "<td>pollution</td>\n",
    "<td>democrat</td>\n",
    "<td>republican</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ENVIRONMENT</td>\n",
    "<td>5</td>\n",
    "<td>7</td>\n",
    "<td>21</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CONGRESS</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>19</td>\n",
    "<td>17</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We change values from frequencies to probabilities by dividing them by sums in rows, which turns each row into probability distribution.\n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<caption>Matrix&nbsp;<strong>H</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>categories</td>\n",
    "<td>air</td>\n",
    "<td>water</td>\n",
    "<td>pollution</td>\n",
    "<td>democrat</td>\n",
    "<td>republican</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ENVIRONMENT</td>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.64</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CONGRESS</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0.53</td>\n",
    "<td>0.47</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Now we create another matrix that contains probability distribution for categories within each document that looks like follows:\n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<caption>Matrix&nbsp;<strong>W</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>documents</td>\n",
    "<td>ENVIRONMENT</td>\n",
    "<td>CONGRESS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 1</td>\n",
    "<td>1.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 2</td>\n",
    "<td>1.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 3</td>\n",
    "<td>0.0</td>\n",
    "<td>1.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 4</td>\n",
    "<td>0.0</td>\n",
    "<td>1.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 5</td>\n",
    "<td>0.6</td>\n",
    "<td>0.4</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "It shows that top two documents speak about environment, next two about congress and last document about both. Ratios 0.6 and 0.4 for the last document are defined by 3 words from environment category and 2 words from congress category. Now we multiply both matrices and compare the result with original data but in a normalized form. Normalization in this case is division of each row by the sum of all elements in rows. The comparison is shown side-by-side below:\n",
    "\n",
    "<table cellpadding=\"10\" style=\"font-family:'Times New Roman'\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "<table border=\"\" cellpadding=\"3\">\n",
    "<caption>Product of&nbsp;<strong>W * H</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.64</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.64</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.53</td>\n",
    "<td>0.47</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.53</td>\n",
    "<td>0.47</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.09</td>\n",
    "<td>0.13</td>\n",
    "<td>0.38</td>\n",
    "<td>0.21</td>\n",
    "<td>0.19</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</td>\n",
    "<td>\n",
    "<table border=\"\" cellpadding=\"3\">\n",
    "<caption>Normalized data&nbsp;<strong>N</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0.23</td>\n",
    "<td>0.15</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.06</td>\n",
    "<td>0.24</td>\n",
    "<td>0.70</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.48</td>\n",
    "<td>0.52</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.61</td>\n",
    "<td>0.39</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "The correlation is obvious. The problem definition is to find constrained matrices W and H (given the number of categories), product of which is the best match with normalized data N. When approximation is found matrix H will contain sought categories.\n",
    "\n",
    "**Formally**, we are trying to minimize this:\n",
    "\n",
    "$$ \\|\\mathbf{N} - \\mathbf{WH}\\|^2_F $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "# Fit the NMF model\n",
    "nmf = decomposition.NMF(n_components=6)\n",
    "nmf.fit(tfidf)\n",
    "W = nmf.transform(tfidf)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inverse the vectorizer vocabulary to be able\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-53-b6e5310c7540>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-53-b6e5310c7540>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print feature_names[i] for i in H[0].argsort[:-21:-1]\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print feature_names[i] for i in H[0].argsort[:-21:-1]  # verifica import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3660, 6495, 1444, 7639, 4609, 8788,  786, 2574,  840, 4827, 5295,\n",
       "       5039, 7261, 9831,  453, 8778, 7410, 5457, 3206, 4947])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0].argsort()[:-21:-1] # -1 per prendere gli ultimi 20 termini in ordine inverso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "god,people,christian,say,jesus,think,believe,don,bible,know,make,life,religion,write,armenian,thing,right,mean,faith,law\n",
      "\n",
      "Topic #1:\n",
      "edu,game,team,year,article,write,car,university,good,post,player,win,play,com,think,organization university,nntp post,host,nntp,post host\n",
      "\n",
      "Topic #2:\n",
      "key,com,chip,clipper,encryption,netcom,government,netcom com,escrow,access,clipper chip,use,communication,phone,algorithm,public,security,crypto,digex,access digex\n",
      "\n",
      "Topic #3:\n",
      "window,use,file,drive,card,com,problem,program,thank,edu,driver,disk,run,help,work,scsi,graphic,version,software,color\n",
      "\n",
      "Topic #4:\n",
      "pitt,pitt edu,geb,gordon bank,gordon,bank,geb pitt,edu gordon,edu,cadre,bank skepticism,chastity intellect,skepticism chastity,cadre dsl,shameful surrender,dsl,surrender soon,dsl pitt,intellect geb,geb cadre\n",
      "\n",
      "Topic #5:\n",
      "edu,cleveland,cwru,cwru edu,ohio,ohio state,freenet,cleveland freenet,freenet edu,state edu,state,case western,reserve university,western reserve,organization case,western,reserve,magnus,university cleveland,host\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(H):\n",
    "    print \"Topic #%d:\" % topic_idx\n",
    "    print \",\".join([feature_names[i] for i in topic.argsort()[:-21:-1]])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import lda\n",
    "\n",
    "# Fit the NMF model\n",
    "lda = decomposition.LatentDirichletAllocation(n_topics=6)\n",
    "lda.fit(tfidf)\n",
    "W = lda.transform(tfidf)\n",
    "H = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "com,edu,write,people,say,article,think,god,don,make,just,know,good,like,post,time,car,use,right,thing\n",
      "\n",
      "Topic #1:\n",
      "edu,window,com,use,university,post,thank,host,game,nntp,nntp post,post host,file,card,write,mail,know,problem,run,drive\n",
      "\n",
      "Topic #2:\n",
      "cwru,cwru edu,cleveland,utexas,utexas edu,cleveland freenet,freenet edu,udel,udel edu,freenet,umich,umich edu,rpi,case western,western reserve,reserve university,organization case,rpi edu,umn,reserve\n",
      "\n",
      "Topic #3:\n",
      "scsi,washington edu,ide,washington,duke,controller,isa,oracle,duke edu,vlb,bus,nyx,ingr,irq,ingr com,jumper,ncsu,ncsu edu,carson,eisa\n",
      "\n",
      "Topic #4:\n",
      "israel,israeli,cmu,purdue,cmu edu,arab,sandvik,virginia,andrew cmu,jew,indiana,columbia,purdue edu,columbia edu,ecn,uci,umd,umd edu,edu,indiana edu\n",
      "\n",
      "Topic #5:\n",
      "armenian,ohio state,caltech,ohio,caltech edu,state edu,stratus,gatech,magnus,turkish,magnus ohio,gatech edu,stratus com,keith,prism,sgi,institute technology,cco,cco caltech,turk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(H):\n",
    "    print \"Topic #%d:\" % topic_idx\n",
    "    print \",\".join([feature_names[i] for i in topic.argsort()[:-21:-1]])\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
